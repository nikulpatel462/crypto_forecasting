{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle - G-Research Crypto Forecasting | Model Fitting\n",
    "\n",
    "This is the final notebook for submitting predictions. Copy over results from the previous two notebooks for our final model construction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! conda update scikit-learn -y \n",
    "! pip install -e lib/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.0G\n",
      "-rw-rw-r-- 1 jupyter    1000  444 Nov  2 15:41 asset_details.csv\n",
      "-rw-rw-r-- 1 jupyter    1000  406 Nov  2 15:41 example_sample_submission.csv\n",
      "-rw-rw-r-- 1 jupyter    1000 5.8K Nov  2 15:41 example_test.csv\n",
      "-rw-rw-r-- 1 jupyter    1000 1.2G Nov  6 21:39 g-research-crypto-forecasting.zip\n",
      "drwxrwxr-x 3 jupyter    1000 4.0K Dec 21 12:41 gresearch_crypto\n",
      "drwxr-xr-x 2 jupyter jupyter 4.0K Dec 19 13:05 jupyter\n",
      "drwxrwxr-x 6 jupyter    1000 4.0K Dec 21 15:29 lib\n",
      "drwxrwxr-x 2 jupyter    1000 4.0K Dec  6 17:50 processed_data\n",
      "-rw-r--r-- 1 root    root     396 Dec 21 17:48 submission.csv\n",
      "-rw-rw-r-- 1 jupyter    1000 232M Nov  2 15:41 supplemental_train.csv\n",
      "-rw-rw-r-- 1 jupyter    1000 2.7G Nov  2 15:42 train.csv\n"
     ]
    }
   ],
   "source": [
    "!ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = False # for lgbm model training\n",
    "\n",
    "device_type = \"gpu\" if USE_GPU else \"cpu\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/gresearch_crypto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gresearch_crypto\n",
    "env = gresearch_crypto.make_env()\n",
    "\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import lightgbm\n",
    "\n",
    "from lib.models import PoolVotingRegressor, PoolRegressor\n",
    "from lib.features import all_feats\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "\n",
    "clusters = {\n",
    "    0: (4, 8, 10, 11),\n",
    "    2: (0, 3, 12, 7), # move 7 in this cluster\n",
    "    3: (2, 5, 13),\n",
    "    4: (1, 6, 9),\n",
    "} # arbitrary cluster labels\n",
    "\n",
    "\n",
    "\n",
    "final_allocations = {\n",
    "    'pool_lasso': 0.13803354577335214,\n",
    "    'pool_LGBM': 0.10673472669853526,\n",
    "    'single_lasso': 0.16562975805884267,\n",
    "    'single_LGBM': 0.14721038027645952,\n",
    "    'pool_all_lasso': 0.21147431517754645,\n",
    "    'pool_all_LGBM': 0.23091727401526385,\n",
    "}\n",
    "\n",
    "all_assetids = list(range(14))\n",
    "\n",
    "pool_params = {\n",
    "    \"lasso\": {\n",
    "        \"model\": Lasso(),\n",
    "        \"params\": {\"alpha\": 0.0022222223000000004, \"fit_intercept\": False},\n",
    "    },\n",
    "    \"LGBM\": {\n",
    "        \"model\": lightgbm.LGBMRegressor(device_type=device_type),\n",
    "        \"params\": {\"learning_rate\": 0.01, \"lambda_l1\": 0.0, \"n_estimators\": 400, \"alpha\": 3}\n",
    "    },\n",
    "}\n",
    "\n",
    "single_params = {\n",
    "    \"lasso\": {\n",
    "        \"model\": Lasso(),\n",
    "        \"params\": {\"alpha\": 0.011111111188888889, \"fit_intercept\": False},\n",
    "    },\n",
    "    \"LGBM\": {\n",
    "        \"model\": lightgbm.LGBMRegressor(device_type=device_type),\n",
    "        \"params\": {\"learning_rate\": 0.01, \"lambda_l1\": 0.03, \"n_estimators\": 100, \"alpha\": 3}\n",
    "    },\n",
    "}\n",
    "\n",
    "all_params = {\n",
    "    \"lasso\": {\n",
    "        \"model\": Lasso(),\n",
    "        \"params\": {\"alpha\": 0.016733333333333333, \"fit_intercept\": False},\n",
    "    },\n",
    "    \"LGBM\": {\n",
    "        \"model\": lightgbm.LGBMRegressor(device_type=device_type),\n",
    "        \"params\": {\"learning_rate\": 0.02, \"lambda_l1\": 0.01, \"n_estimators\": 200}\n",
    "    },\n",
    "}\n",
    "\n",
    "param_dict = {\n",
    "    \"pool\": {\n",
    "        \"params\": pool_params,\n",
    "        \"clusters\": clusters,\n",
    "    },\n",
    "    \"single\": {\n",
    "        \"params\": single_params,\n",
    "        \"clusters\": {k: [k] for k in all_assetids},\n",
    "    },\n",
    "    \"pool_all\": {\n",
    "        \"params\": all_params,\n",
    "        \"clusters\": {-1: all_assetids},\n",
    "    },\n",
    "}\n",
    "\n",
    "all_models = {\n",
    "    f\"{setup}_{model_type}\": PoolRegressor(model[\"model\"].set_params(**model[\"params\"]), clusters=model_dict[\"clusters\"])\n",
    "    for setup, model_dict in param_dict.items()\n",
    "    for model_type, model in model_dict[\"params\"].items()\n",
    "}\n",
    "models_list = [(k, model) for k, model in all_models.items()]\n",
    "model_weight_list = [final_allocations[k] for k in all_models] # ensure in same order\n",
    "\n",
    "voting_model = PoolVotingRegressor(estimators=models_list, weights=model_weight_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.features import all_feats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "ad = pd.read_csv(\"asset_details.csv\").set_index(\"Asset_ID\")\n",
    "weights = ad[\"Weight\"]\n",
    "weights /= weights.sum()\n",
    "\n",
    "N_TO_KEEP = 10000\n",
    "\n",
    "def last_n_ts_df(df, lookback):\n",
    "    \"\"\"Returns the last rows of df where the timestamp is in the last n of all\n",
    "    timestamps. This is to concatenate with new data provided by the API so that\n",
    "    rolling calculations can be performed.\n",
    "    \n",
    "    Warning: assumes df is ordered by timestamps, and could return more data than\n",
    "    requested.\n",
    "    \"\"\"\n",
    "    n_assets = 14\n",
    "    return df.iloc[-(n_assets * lookback + 500):]\n",
    "\n",
    "\n",
    "def concat_old_new(old_data, new_data):\n",
    "    \"\"\"Concatenate old and new dfs for feature construction. Ensures\n",
    "    any overlapping timestamps + assetids in the old df are discarded.\n",
    "    \"\"\"\n",
    "    return pd.concat([old_data, new_data.drop(columns=\"row_id\")], ignore_index=True)\n",
    "\n",
    "def subset_test_index(data, orig_data):\n",
    "    \"\"\"Subset the prepred data df on the original test timestamps + assetids\"\"\"\n",
    "    orig_index = pd.MultiIndex.from_frame(orig_data[[\"timestamp\", \"Asset_ID\"]])\n",
    "    return data.loc[orig_index]\n",
    "\n",
    "\n",
    "def join_rowids(preds, orig_test):\n",
    "    \"\"\"Join our predictions df with the rowids in the supplied test data df\"\"\"\n",
    "    orig_join_on = test_df[[\"timestamp\", \"Asset_ID\", \"row_id\"]].set_index([\"timestamp\", \"Asset_ID\"])\n",
    "    return preds.join(orig_join_on).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def predict_loop(model, weights, prev_data, new_data, sample_pred_df, n_to_keep):\n",
    "    \"\"\"Function for looping over in env.iter_test():\n",
    "    - Concatenate previous + new data\n",
    "    - Cache last n rows of this df\n",
    "    - Calculate new features\n",
    "    - Drop rows to match the original training timestamps + asset ids\n",
    "    - Calculate predictions on this subset\n",
    "    - Join with the given row ids in the sample predictions df\n",
    "    \n",
    "    Returns: last n rows from prev + new data, predictions df\n",
    "    \"\"\"\n",
    "    concat_data = concat_old_new(prev_data, new_data)\n",
    "    last_n = last_n_ts_df(concat_data, n_to_keep)\n",
    "    feats = all_feats(concat_data, weights, fillna_val=0, include_target=False).stack()\n",
    "    feats = subset_test_index(feats, new_data)\n",
    "    preds = model.predict(feats).rename(\"Target\").to_frame()\n",
    "    return last_n, join_rowids(preds, new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = pd.read_csv(\"train.csv\")\n",
    "# last_n = last_n_ts_df(train_data, N_TO_KEEP)\n",
    "# train = all_feats(train_data, weights, fillna_val=np.nan, include_target=True).stack().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open(\"/tmp/mod.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(voting_model, f)\n",
    "    \n",
    "# with open(\"/tmp/d.pkl\", \"wb\") as f:\n",
    "#     pickle.dump((train, last_n), f)\n",
    "    \n",
    "with open(\"/tmp/mod.pkl\", \"rb\") as f:\n",
    "    voting_model = pickle.load(f)\n",
    "    \n",
    "with open(\"/tmp/d.pkl\", \"rb\") as f:\n",
    "    (train, last_n) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14424/1354493953.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvoting_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# not needed so delete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "voting_model.fit(train)\n",
    "\n",
    "train.drop(train.index, inplace=True) # not needed so delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n"
     ]
    }
   ],
   "source": [
    "preds_ = []\n",
    "for (test_df, sample_prediction_df) in iter_test:\n",
    "    last_n, preds = predict_loop(voting_model, weights, last_n, test_df, sample_prediction_df, 4000)\n",
    "    env.predict(sample_prediction_df)\n",
    "    preds_.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
