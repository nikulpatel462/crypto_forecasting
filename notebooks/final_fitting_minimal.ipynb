{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! conda update scikit-learn -y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       ".datatable .frame thead tr.colnames {  background-image: url('data:image/png;base64,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');  background-repeat: repeat-x;  background-size: 14px;  height: 28px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import warnings\n",
    "import os\n",
    "import logging\n",
    "import pickle\n",
    "from functools import reduce\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, clone, RegressorMixin\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "import lightgbm\n",
    "\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "USE_GPU = False # for lgbm model training\n",
    "\n",
    "device_type = \"gpu\" if USE_GPU else \"cpu\"\n",
    "\n",
    "\n",
    "# sys.path.append(\"/home/gresearch_crypto\")\n",
    "sys.path.append(\"gresearch_crypto\")\n",
    "\n",
    "\n",
    "import gresearch_crypto\n",
    "env = gresearch_crypto.make_env()\n",
    "\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "### Functions for data preparatino + prediction loop\n",
    "####################################################################################################\n",
    "\n",
    "def chunk_list(l, chunk_size):\n",
    "    \"\"\"Iterate over a list in chunks\"\"\"\n",
    "    for x in range(0, len(l), chunk_size):\n",
    "        yield l[x : x + chunk_size]\n",
    "\n",
    "\n",
    "def bar_feats_minimal(df):\n",
    "    \"\"\"Augment the given dataframe in place with some features for each bar.\"\"\"\n",
    "    midpoint = (df[\"Open\"] + df[\"Close\"]) / 2\n",
    "    feats = {\n",
    "        \"rel_avg\": df[\"VWAP\"] / midpoint,\n",
    "        \"avg_t_size\": (df[\"Volume\"] / df[\"Count\"])\n",
    "        ** (1 / 10),  # average # of units per transaction\n",
    "        \"dollar_vol\": np.log(df[\"Volume\"] * df[\"VWAP\"]),  # dollar volume traded\n",
    "        \"rel_dev\": ((df[\"High\"] - df[\"Low\"]) / midpoint) ** (1 / 3),\n",
    "        \"shadow_diff\": (df[\"High\"] + df[\"Low\"]) / (2 * midpoint) - 1,\n",
    "    }\n",
    "    for name, feat in feats.items():\n",
    "        df.loc[:, name] = feat\n",
    "\n",
    "\n",
    "\n",
    "def ts_feats_minimal(df, window, price_mom_windows, include_target=True):\n",
    "    \"\"\"Add rolling z-score features including price momentum features and the target + target scale.\n",
    "    Assumes index is timestamps + Asset_IDs\n",
    "\n",
    "    Warning: changes input df in-place to save memory\n",
    "    \"\"\"\n",
    "    to_z_score = [\n",
    "        \"rel_avg\",\n",
    "        \"avg_t_size\",\n",
    "        \"shadow_diff\",\n",
    "        \"dollar_vol\",\n",
    "        \"rel_dev\",\n",
    "    ]\n",
    "    df_subset = df[to_z_score]\n",
    "    \n",
    "    target = df[\"Target\"] if include_target else None\n",
    "    close_prices = df[[\"Close\"]]    \n",
    "\n",
    "    log_close_grp = close_prices.groupby(level=\"Asset_ID\", as_index=False)\n",
    "\n",
    "    for mom_window in price_mom_windows:\n",
    "        feat_name = f\"price_mom_{mom_window}\"\n",
    "        df_subset.loc[:, feat_name] = log_close_grp.diff(mom_window)[\"Close\"]\n",
    "\n",
    "    min_periods = max(1, window // 10)\n",
    "    df_grp = (\n",
    "        df_subset\n",
    "        .groupby(level=\"Asset_ID\", as_index=False)\n",
    "        .rolling(window, min_periods=min_periods)\n",
    "    )\n",
    "\n",
    "    roll_mean = df_grp.mean().drop(columns=\"Asset_ID\").fillna(0).reindex(index=df.index)\n",
    "    roll_std = df_grp.std().drop(columns=\"Asset_ID\").ffill().fillna(1).reindex(index=df.index)\n",
    "\n",
    "    norm_feats = ((df_subset - roll_mean) / roll_std).rename(\n",
    "        mapper=lambda x: \"roll_\" + x, axis=\"columns\"\n",
    "    )\n",
    "\n",
    "    norm_feats.loc[:, \"target_scale\"] = roll_std[\"price_mom_15\"]\n",
    "\n",
    "    if include_target:  # FIXME: potentially confusing target naming convention\n",
    "        norm_feats.loc[:, \"scaled_target\"] = target / norm_feats[\"target_scale\"]\n",
    "        norm_feats.loc[:, \"target\"] = target\n",
    "\n",
    "    return norm_feats\n",
    "\n",
    "\n",
    "\n",
    "def all_feats_minimal(df, include_target=True):\n",
    "    \"\"\"Minimal version of all_feats\"\"\"\n",
    "    price_mom_windows = (1, 5, 15, 80)\n",
    "    window = 300\n",
    "\n",
    "    bar_feats_minimal(df)  # augment in-place with bar features\n",
    "    df.drop(\n",
    "        columns=[\"Count\", \"Open\", \"High\", \"Low\", \"Volume\", \"VWAP\"], inplace=True\n",
    "    )  # drop unused columns\n",
    "\n",
    "    df.set_index([\"timestamp\", \"Asset_ID\"], inplace=True)\n",
    "\n",
    "    return ts_feats_minimal(df, window, price_mom_windows, include_target)\n",
    "\n",
    "\n",
    "\n",
    "class ResultCacher:\n",
    "    \"\"\"Helper class to pickle and load a series of results to a given directory.\n",
    "    \n",
    "    Note: saves as pickles rather than eg parquet for low RAM requirements when reading files\n",
    "    and for low file sizes (compared to csv).\n",
    "    \"\"\"\n",
    "    def __init__(self, save_path):\n",
    "        self.save_path = save_path # directory to cache intermediate results\n",
    "        self.result_paths = [] # to keep track of cached files\n",
    "        \n",
    "        self.create_save_folder()\n",
    "        \n",
    "    def create_save_folder(self):\n",
    "        os.makedirs(self.save_path)\n",
    "        \n",
    "    def get_save_path(self, filename):\n",
    "        return os.path.join(self.save_path, filename)\n",
    "        \n",
    "    def cache_result(self, result, filename):\n",
    "        \"\"\"Pickles result to given filename in save_path\"\"\"\n",
    "        file_save_path = self.get_save_path(filename)\n",
    "        logger.info(f\"Saving result to {file_save_path}\")\n",
    "        with open(file_save_path, \"wb\") as f:\n",
    "            pickle.dump(result, f)\n",
    "            \n",
    "        self.result_paths.append(file_save_path)\n",
    "            \n",
    "    def load_all_results(self):\n",
    "        \"\"\"Loads all cached results and returns as a list\"\"\"\n",
    "        results = []\n",
    "        for path in self.result_paths:\n",
    "            with open(path, \"rb\") as f:\n",
    "                results.append(pickle.load(f))\n",
    "        return results\n",
    "\n",
    "\n",
    "def chunk_ts_feats(full_df, n_splits):\n",
    "    \"\"\"Run all_feats_minimal but chunk over the original df, save intermediate results to disk,\n",
    "    and concatenate all results after finished to avoid memory issues for the full data preparation.\n",
    "    \n",
    "    Note: chunks over timestamps, not array indices to reduce potential ordering bugs.\n",
    "    \"\"\"\n",
    "    include_target = True\n",
    "    all_timestamps = np.sort(full_df[\"timestamp\"].unique())\n",
    "    chunk_size = len(all_timestamps) // n_splits + 1000 # avoid rounding issue for last chunk\n",
    "    \n",
    "    result_cacher = ResultCacher(\"/tmp/feat_cache\")\n",
    "    \n",
    "    for i, time_chunk in enumerate(chunk_list(all_timestamps, chunk_size)):   \n",
    "        df_chunk = full_df.loc[full_df.timestamp.isin(time_chunk)]\n",
    "        feat_chunk = all_feats_minimal(df_chunk, include_target=True).dropna()\n",
    "        result_cacher.cache_result(feat_chunk, f\"feat_chunk_{i}.pkl\")\n",
    "        \n",
    "    all_feats = result_cacher.load_all_results() # FIXME: does not release \n",
    "    return pd.concat(all_feats)\n",
    "        \n",
    "        \n",
    "\n",
    "def last_n_ts_df(df, lookback, buffer=100):\n",
    "    \"\"\"Returns the last rows of df where the timestamp is in the last n of all\n",
    "    timestamps. This is to concatenate with new data provided by the API so that\n",
    "    rolling calculations can be performed.\n",
    "\n",
    "    Warning: assumes df is ordered by timestamps, and could return more data than\n",
    "    requested.\n",
    "    \"\"\"\n",
    "    n_assets = 14\n",
    "    return df.iloc[-(n_assets * lookback + buffer) :]\n",
    "\n",
    "\n",
    "def concat_old_new(old_data, new_data):\n",
    "    \"\"\"Concatenate old and new dfs for feature construction. Ensures\n",
    "    any overlapping timestamps + assetids in the old df are discarded.\n",
    "    \"\"\"\n",
    "    return pd.concat([old_data, new_data.drop(columns=\"row_id\")], ignore_index=True)\n",
    "\n",
    "\n",
    "def subset_test_index(data, orig_data):\n",
    "    \"\"\"Subset the prepred data df on the original test timestamps + assetids\"\"\"\n",
    "    orig_index = pd.MultiIndex.from_frame(orig_data[[\"timestamp\", \"Asset_ID\"]])\n",
    "    return data.loc[orig_index]\n",
    "\n",
    "\n",
    "def join_rowids(preds, orig_test):\n",
    "    \"\"\"Join our predictions df with the rowids in the supplied test data df\"\"\"\n",
    "    orig_join_on = orig_test[[\"timestamp\", \"Asset_ID\", \"row_id\"]].set_index(\n",
    "        [\"timestamp\", \"Asset_ID\"]\n",
    "    )\n",
    "    return preds.join(orig_join_on).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def predict_loop(model, prev_data, new_data, sample_pred_df, n_to_keep):\n",
    "    \"\"\"Function for looping over in env.iter_test():\n",
    "    - Concatenate previous + new data\n",
    "    - Cache last n rows of this df\n",
    "    - Calculate new features\n",
    "    - Drop rows to match the original training timestamps + asset ids\n",
    "    - Calculate predictions on this subset\n",
    "    - Join with the given row ids in the sample predictions df\n",
    "\n",
    "    Returns: last n rows from prev + new data, predictions df\n",
    "    \"\"\"\n",
    "    concat_data = concat_old_new(prev_data, new_data)\n",
    "    last_n = last_n_ts_df(concat_data, n_to_keep)\n",
    "    feats = all_feats_minimal(concat_data, include_target=False)\n",
    "    feats = subset_test_index(feats, new_data).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    preds = model.predict(feats).rename(\"Target\").to_frame()\n",
    "    return last_n, join_rowids(preds, new_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "### Functions for fitting\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "def feature_names(data_cols):\n",
    "    \"\"\"Take single/multiindex columns from a pandas df of features + targets\n",
    "    and return only the feature names.\n",
    "    \"\"\"\n",
    "    if isinstance(data_cols, pd.MultiIndex):\n",
    "        data_cols = data_cols.get_level_values(0).unique()\n",
    "\n",
    "    return [k for k in data_cols if \"target\" not in k]\n",
    "\n",
    "\n",
    "def get_xy_arrays(data_df):\n",
    "    \"\"\"Returns a tuple of numpy arrays: features, scaled targets\"\"\"\n",
    "    features = feature_names(data_df.columns)\n",
    "    try:\n",
    "        target = data_df[\"scaled_target\"].values\n",
    "    except:\n",
    "        target = None\n",
    "    return data_df[features].values, target\n",
    "\n",
    "\n",
    "def scale_predictions(model, X, y_scale):\n",
    "    \"\"\"Undo the vol normalisation for the targets to get predictions\n",
    "    for actual returns.\n",
    "    \"\"\"\n",
    "    return model.predict(X) * y_scale\n",
    "\n",
    "\n",
    "def weighted_correlation(a, b, weights):\n",
    "    \"\"\"Evaluation metric copied from the discussion page\n",
    "    https://www.kaggle.com/c/g-research-crypto-forecasting/discussion/291845\n",
    "\n",
    "    Excpects columns of actual targets, predictions and asset weights\n",
    "\n",
    "    Args:\n",
    "    - a, b: the actual and predicted weights\n",
    "    - weights: the associated asset weights\n",
    "    \"\"\"\n",
    "    w = np.ravel(weights)\n",
    "    a = np.ravel(a)\n",
    "    b = np.ravel(b)\n",
    "\n",
    "    sum_w = np.sum(w)\n",
    "    mean_a = np.sum(a * w) / sum_w\n",
    "    mean_b = np.sum(b * w) / sum_w\n",
    "    var_a = np.sum(w * np.square(a - mean_a)) / sum_w\n",
    "    var_b = np.sum(w * np.square(b - mean_b)) / sum_w\n",
    "\n",
    "    cov = np.sum((a * b * w)) / np.sum(w) - mean_a * mean_b\n",
    "    corr = cov / np.sqrt(var_a * var_b)\n",
    "\n",
    "    return corr\n",
    "\n",
    "\n",
    "\n",
    "def score_from_df(pred_df, X):\n",
    "    \"\"\"Ensure indices are aligned before calculating the weighted correlation\n",
    "    on the given predictions df.\n",
    "\n",
    "    Note: the score will be nan if one df contains index values not found in the other.\n",
    "    This likely indicates a bug in generating predictions since the predictions should\n",
    "    have been calculated using X, so the indices should be the same in some order.\n",
    "    \"\"\"\n",
    "    pred_df_reindexed = pred_df.reindex(index=X.index)\n",
    "    return weighted_correlation(\n",
    "        X.target.values,\n",
    "        pred_df_reindexed.values,\n",
    "        X.target_weight.values,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def score_pool_model(model, X):\n",
    "    \"\"\"Convenience function for generating predictions and passing to score_from_df\"\"\"\n",
    "    preds = model.predict(X)\n",
    "    return score_from_df(preds, X)\n",
    "\n",
    "\n",
    "class PoolRegressor(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Helper class for fitting pool models.\n",
    "\n",
    "    Notes:\n",
    "    - This depends on the input X being a pandas df. sklearn (deliberately) tends not\n",
    "    to work well with pandas, but we don't use sklearn functionality extensively here\n",
    "    and what we do use will be okay (for indexing sklearn seems to take care of things,\n",
    "    see: https://github.com/scikit-learn/scikit-learn/blob/0d378913be6d7e485b792ea36e9268be31ed52d0/sklearn/utils/__init__.py#L307)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_model, clusters: dict):\n",
    "        self.base_model = base_model\n",
    "        self.clusters = clusters\n",
    "        self.asset_ids_ = reduce(lambda x, y: [*x, *y], clusters.values())\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, X, y=None, **fit_kwargs):\n",
    "        \"\"\"Expects a long df using the \"targets\" column as the targets, and\n",
    "        any column without \"target\" in the name is used as a feature. Fit one\n",
    "        model for each given cluster.\n",
    "\n",
    "        Note: the case where each cluster has size 1 is the single asset model.\n",
    "        \"\"\"\n",
    "        self.models_ = {}\n",
    "        for cluster, asset_ids in self.clusters.items():\n",
    "            X_subset, y_subset = get_xy_arrays(X.loc[(slice(None), list(asset_ids)), :])\n",
    "            model_clone = clone(self.base_model)\n",
    "            model_clone.fit(\n",
    "                X_subset, y_subset, **fit_kwargs\n",
    "            )  # fit separately for compatibility with Keras\n",
    "            self.models_[cluster] = model_clone\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X) -> pd.DataFrame:\n",
    "        \"\"\"Take a long df of features and return a wide df of predictions\n",
    "        with asset_ids as columns.\n",
    "        \"\"\"\n",
    "        preds = []\n",
    "        for cluster, asset_ids in self.clusters.items():\n",
    "            X_subset = X.loc[(slice(None), asset_ids), :]\n",
    "            cluster_preds = scale_predictions(\n",
    "                self.models_[cluster], get_xy_arrays(X_subset)[0], X_subset.target_scale\n",
    "            )\n",
    "            preds.append(pd.Series(cluster_preds, index=X_subset.index))\n",
    "            # asset_preds = self.models_[asset_id].predict(get_xy_arrays(X_subset)[0])\n",
    "            # asset_preds = pd.Series(asset_preds, index=X_subset.index)\n",
    "            # preds[asset_id] = asset_preds * X_subset.target_scale # scale back to returns predictions\n",
    "\n",
    "        return pd.concat(preds).reindex(index=X.index)  # same order as input df\n",
    "\n",
    "    def score(self, X, y=None):\n",
    "        \"\"\"Return the weighted correlation between all predictions\"\"\"\n",
    "        return score_pool_model(self, X)\n",
    "\n",
    "\n",
    "class PoolVotingRegressor(RegressorMixin):\n",
    "    \"\"\"Wrapper around VotingRegressor intended for use with PoolRegressors.\n",
    "    In particular:\n",
    "    - change the default scoring function to weighted regression\n",
    "    - keep the original pandas indices to predictions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, estimators, weights=None):\n",
    "        self.estimators = estimators\n",
    "        self.weights = weights\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        empty_y = np.empty_like(X.iloc[:, 0])\n",
    "        self.voting_regressor_ = VotingRegressor(\n",
    "            estimators=self.estimators, weights=self.weights\n",
    "        )\n",
    "        self.voting_regressor_.fit(X, empty_y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Adds the original pandas index of X to the output of the wrapped\n",
    "        VotingRegressor.\n",
    "        \"\"\"\n",
    "        preds = self.voting_regressor_.predict(X)\n",
    "        return pd.Series(preds, index=X.index)\n",
    "\n",
    "    def score(self, X, y=None):  # FIXME: duplication from PoolRegressor\n",
    "        \"\"\"Return the weighted correlation between all predictions\"\"\"\n",
    "        return score_pool_model(self, X)\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "### Hardcoded model config\n",
    "####################################################################################################\n",
    "\n",
    "clusters = {\n",
    "    0: (4, 8, 10, 11),\n",
    "    2: (0, 3, 12, 7), # move 7 in this cluster\n",
    "    3: (2, 5, 13),\n",
    "    4: (1, 6, 9),\n",
    "} # arbitrary cluster labels\n",
    "\n",
    "\n",
    "\n",
    "final_allocations = {\n",
    "    'pool_lasso': 0.13803354577335214,\n",
    "    'pool_LGBM': 0.10673472669853526,\n",
    "    'single_lasso': 0.16562975805884267,\n",
    "    'single_LGBM': 0.14721038027645952,\n",
    "    'pool_all_lasso': 0.21147431517754645,\n",
    "    'pool_all_LGBM': 0.23091727401526385,\n",
    "}\n",
    "\n",
    "all_assetids = list(range(14))\n",
    "\n",
    "pool_params = {\n",
    "    \"lasso\": {\n",
    "        \"model\": Lasso(),\n",
    "        \"params\": {\"alpha\": 0.0022222223000000004, \"fit_intercept\": False},\n",
    "    },\n",
    "    \"LGBM\": {\n",
    "        \"model\": lightgbm.LGBMRegressor(device_type=device_type),\n",
    "        \"params\": {\"learning_rate\": 0.01, \"lambda_l1\": 0.0, \"n_estimators\": 400, \"alpha\": 3}\n",
    "    },\n",
    "}\n",
    "\n",
    "single_params = {\n",
    "    \"lasso\": {\n",
    "        \"model\": Lasso(),\n",
    "        \"params\": {\"alpha\": 0.011111111188888889, \"fit_intercept\": False},\n",
    "    },\n",
    "    \"LGBM\": {\n",
    "        \"model\": lightgbm.LGBMRegressor(device_type=device_type),\n",
    "        \"params\": {\"learning_rate\": 0.01, \"lambda_l1\": 0.03, \"n_estimators\": 100, \"alpha\": 3}\n",
    "    },\n",
    "}\n",
    "\n",
    "all_params = {\n",
    "    \"lasso\": {\n",
    "        \"model\": Lasso(),\n",
    "        \"params\": {\"alpha\": 0.016733333333333333, \"fit_intercept\": False},\n",
    "    },\n",
    "    \"LGBM\": {\n",
    "        \"model\": lightgbm.LGBMRegressor(device_type=device_type),\n",
    "        \"params\": {\"learning_rate\": 0.02, \"lambda_l1\": 0.01, \"n_estimators\": 200}\n",
    "    },\n",
    "}\n",
    "\n",
    "param_dict = {\n",
    "    \"pool\": {\n",
    "        \"params\": pool_params,\n",
    "        \"clusters\": clusters,\n",
    "    },\n",
    "    \"single\": {\n",
    "        \"params\": single_params,\n",
    "        \"clusters\": {k: [k] for k in all_assetids},\n",
    "    },\n",
    "    \"pool_all\": {\n",
    "        \"params\": all_params,\n",
    "        \"clusters\": {-1: all_assetids},\n",
    "    },\n",
    "}\n",
    "\n",
    "all_models = {\n",
    "    f\"{setup}_{model_type}\": PoolRegressor(model[\"model\"].set_params(**model[\"params\"]), clusters=model_dict[\"clusters\"])\n",
    "    for setup, model_dict in param_dict.items()\n",
    "    for model_type, model in model_dict[\"params\"].items()\n",
    "}\n",
    "models_list = [(k, model) for k, model in all_models.items()]\n",
    "model_weight_list = [final_allocations[k] for k in all_models] # ensure in same order\n",
    "\n",
    "voting_model = PoolVotingRegressor(estimators=models_list, weights=model_weight_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "N_TO_KEEP = 1000\n",
    "last_n = last_n_ts_df(train_data, N_TO_KEEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf /tmp/feat_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "train = chunk_ts_feats(train_data, 5).replace([np.inf, -np.inf], np.nan).dropna() # FIXME: np.inf introduced, probably from std dev normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=0.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.PoolVotingRegressor at 0x7f3d8deda390>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"/tmp/model.pkl\", \"wb\") as f:\n",
    "#     pickle.dump([last_n, voting_model], f)\n",
    "\n",
    "N_TO_KEEP = 1000\n",
    "\n",
    "with open(\"/tmp/model.pkl\", \"rb\") as f:\n",
    "    last_n, voting_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_n = last_n.iloc[:-14*100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n",
      "CPU times: user 2.85 s, sys: 4.16 ms, total: 2.86 s\n",
      "Wall time: 1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ps = []\n",
    "for (test_df, sample_prediction_df) in iter_test:\n",
    "    last_n, preds = predict_loop(voting_model, last_n, test_df, sample_prediction_df, N_TO_KEEP)\n",
    "    env.predict(preds)\n",
    "    ps.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>row_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.259892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.952608</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-7.958667</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1485.671085</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.267056</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.192973</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.224531</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-154.013596</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.236358</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.224942</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>362.028820</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.015110</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.014569</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.065194</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.159672</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.354789</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-5.822441</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1430.120558</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.171024</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.155257</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.145678</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-102.899679</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.144895</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.142606</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>272.473030</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.012753</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.011380</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.420062</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.121695</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.453558</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-6.132120</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1068.054470</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.257079</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.113957</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.028645</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-108.910767</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.166733</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.074079</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>287.679337</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.012642</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.011619</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.268011</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.143655</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.980200</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-5.463152</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1160.453427</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.314906</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.109181</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.998398</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-78.720861</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.148565</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.076250</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>121.310736</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.008851</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.008076</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.345724</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Target  row_id\n",
       "0     -0.259892       0\n",
       "1      8.952608       1\n",
       "2     -7.958667       2\n",
       "3  -1485.671085       3\n",
       "4      0.267056       4\n",
       "5      0.192973       5\n",
       "6      1.224531       6\n",
       "7   -154.013596       7\n",
       "8     -0.236358       8\n",
       "9      0.224942       9\n",
       "10   362.028820      10\n",
       "11    -0.015110      11\n",
       "12     0.014569      12\n",
       "13     5.065194      13\n",
       "0     -0.159672      14\n",
       "1      7.354789      15\n",
       "2     -5.822441      16\n",
       "3  -1430.120558      17\n",
       "4      0.171024      18\n",
       "5      0.155257      19\n",
       "6      1.145678      20\n",
       "7   -102.899679      21\n",
       "8     -0.144895      22\n",
       "9      0.142606      23\n",
       "10   272.473030      24\n",
       "11    -0.012753      25\n",
       "12     0.011380      26\n",
       "13     3.420062      27\n",
       "0     -0.121695      28\n",
       "1      6.453558      29\n",
       "2     -6.132120      30\n",
       "3  -1068.054470      31\n",
       "4      0.257079      32\n",
       "5      0.113957      33\n",
       "6      1.028645      34\n",
       "7   -108.910767      35\n",
       "8     -0.166733      36\n",
       "9      0.074079      37\n",
       "10   287.679337      38\n",
       "11    -0.012642      39\n",
       "12     0.011619      40\n",
       "13     2.268011      41\n",
       "0     -0.143655      42\n",
       "1      4.980200      43\n",
       "2     -5.463152      44\n",
       "3  -1160.453427      45\n",
       "4      0.314906      46\n",
       "5      0.109181      47\n",
       "6      0.998398      48\n",
       "7    -78.720861      49\n",
       "8     -0.148565      50\n",
       "9     -0.076250      51\n",
       "10   121.310736      52\n",
       "11    -0.008851      53\n",
       "12     0.008076      54\n",
       "13     2.345724      55"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_model.predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
