{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle - G-Research Crypto Forecasting | Model Fitting\n",
    "\n",
    "This is the final notebook for submitting predictions. Use the previous modelling setup but with two changes:\n",
    "1) Simplify the feature construction by not \"market neutralising\"\n",
    "2) Remove the Lasso models\n",
    "\n",
    "These changes are to help us stay within the 9 hour time limit. We note below the OOS score on the test set isn't too affected by these changes.\n",
    "\n",
    "We also move all functions here to keep the notebook self-contained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       ".datatable .frame thead tr.colnames {  background-image: url('data:image/png;base64,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');  background-repeat: repeat-x;  background-size: 14px;  height: 28px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import warnings\n",
    "import os\n",
    "import logging\n",
    "import pickle\n",
    "import time\n",
    "from functools import reduce\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, clone, RegressorMixin\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "import lightgbm\n",
    "\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "START_TIME = time.time()\n",
    "\n",
    "USE_GPU = False # for lgbm model training\n",
    "\n",
    "device_type = \"gpu\" if USE_GPU else \"cpu\"\n",
    "\n",
    "\n",
    "# sys.path.append(\"/home/gresearch_crypto\")\n",
    "sys.path.append(\"gresearch_crypto\")\n",
    "\n",
    "\n",
    "import gresearch_crypto\n",
    "env = gresearch_crypto.make_env()\n",
    "\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "### Functions for data preparatino + prediction loop\n",
    "####################################################################################################\n",
    "\n",
    "TIME_LIMIT = 8.5 # Time limit in hours for fitting + predictions. After this submit the given sample predictions.\n",
    "\n",
    "def time_elapsed():\n",
    "    \"\"\"Get the elapsed time from when this notebook was started\"\"\"\n",
    "    return time.time() - START_TIME\n",
    "\n",
    "\n",
    "def in_time_limit():\n",
    "    \"\"\"Returns True if the elapsed time from the notebook starting is < 8.5 hours.\n",
    "    Else returns False.\n",
    "    \"\"\"\n",
    "    elapsed_hours = time_elapsed() / 3600\n",
    "    if elapsed_hours < TIME_LIMIT:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def chunk_list(l, chunk_size):\n",
    "    \"\"\"Iterate over a list in chunks\"\"\"\n",
    "    for x in range(0, len(l), chunk_size):\n",
    "        yield l[x : x + chunk_size]\n",
    "\n",
    "\n",
    "def bar_feats_minimal(df):\n",
    "    \"\"\"Augment the given dataframe in place with some features for each bar.\"\"\"\n",
    "    midpoint = (df[\"Open\"] + df[\"Close\"]) / 2\n",
    "    feats = {\n",
    "        \"rel_avg\": df[\"VWAP\"] / midpoint,\n",
    "        \"avg_t_size\": (df[\"Volume\"] / df[\"Count\"])\n",
    "        ** (1 / 10),  # average # of units per transaction\n",
    "        \"dollar_vol\": np.log(df[\"Volume\"] * df[\"VWAP\"]),  # dollar volume traded\n",
    "        \"rel_dev\": ((df[\"High\"] - df[\"Low\"]) / midpoint) ** (1 / 3),\n",
    "        \"shadow_diff\": (df[\"High\"] + df[\"Low\"]) / (2 * midpoint) - 1,\n",
    "    }\n",
    "    for name, feat in feats.items():\n",
    "        df.loc[:, name] = feat\n",
    "\n",
    "\n",
    "\n",
    "def ts_feats_minimal(df, window, price_mom_windows, include_target=True):\n",
    "    \"\"\"Add rolling z-score features including price momentum features and the target + target scale.\n",
    "    Assumes index is timestamps + Asset_IDs\n",
    "\n",
    "    Warning: changes input df in-place to save memory\n",
    "    \"\"\"\n",
    "    to_z_score = [\n",
    "        \"rel_avg\",\n",
    "        \"avg_t_size\",\n",
    "        \"shadow_diff\",\n",
    "        \"dollar_vol\",\n",
    "        \"rel_dev\",\n",
    "    ]\n",
    "    df_subset = df[to_z_score]\n",
    "    \n",
    "    target = df[\"Target\"] if include_target else None\n",
    "    log_close = np.log(df[[\"Close\"]])\n",
    "\n",
    "    log_close_grp = log_close.groupby(level=\"Asset_ID\", as_index=False)\n",
    "\n",
    "    for mom_window in price_mom_windows:\n",
    "        feat_name = f\"price_mom_{mom_window}\"\n",
    "        df_subset.loc[:, feat_name] = log_close_grp.diff(mom_window)[\"Close\"]\n",
    "\n",
    "    min_periods = max(1, window // 10)\n",
    "    df_grp = (\n",
    "        df_subset\n",
    "        .groupby(level=\"Asset_ID\", as_index=False)\n",
    "        .rolling(window, min_periods=min_periods)\n",
    "    )    \n",
    "\n",
    "    norm_feats = df_subset - df_grp.mean().drop(columns=\"Asset_ID\").fillna(0).reindex(index=df.index)\n",
    "    roll_std = (\n",
    "        norm_feats.groupby(level=\"Asset_ID\", as_index=False)\n",
    "        .rolling(window, min_periods=min_periods)\n",
    "        .std()\n",
    "        .drop(columns=\"Asset_ID\")\n",
    "        .ffill()\n",
    "        .fillna(1)\n",
    "        .reindex(index=df.index)\n",
    "    )\n",
    "    norm_feats = norm_feats / roll_std\n",
    "\n",
    "    norm_feats = norm_feats.rename(\n",
    "        mapper=lambda x: \"roll_\" + x, axis=\"columns\"\n",
    "    )\n",
    "\n",
    "    norm_feats.loc[:, \"target_scale\"] = roll_std[\"price_mom_15\"]\n",
    "\n",
    "    if include_target:  # FIXME: potentially confusing target naming convention\n",
    "        norm_feats.loc[:, \"scaled_target\"] = target / norm_feats[\"target_scale\"]\n",
    "        norm_feats.loc[:, \"target\"] = target\n",
    "\n",
    "    return norm_feats\n",
    "\n",
    "\n",
    "\n",
    "def all_feats_minimal(df, include_target=True):\n",
    "    \"\"\"Minimal version of all_feats\"\"\"\n",
    "    price_mom_windows = (1, 5, 15, 80)\n",
    "    window = 600\n",
    "\n",
    "    bar_feats_minimal(df)  # augment in-place with bar features\n",
    "    df.drop(\n",
    "        columns=[\"Count\", \"Open\", \"High\", \"Low\", \"Volume\", \"VWAP\"], inplace=True\n",
    "    )  # drop unused columns\n",
    "\n",
    "    df.set_index([\"timestamp\", \"Asset_ID\"], inplace=True)\n",
    "    \n",
    "    df = df.loc[df.index.duplicated(keep='last')]\n",
    "\n",
    "    return ts_feats_minimal(df, window, price_mom_windows, include_target)\n",
    "\n",
    "\n",
    "\n",
    "class ResultCacher:\n",
    "    \"\"\"Helper class to pickle and load a series of results to a given directory.\n",
    "    \n",
    "    Note: saves as pickles rather than eg parquet for low RAM requirements when reading files\n",
    "    and for low file sizes (compared to csv).\n",
    "    \"\"\"\n",
    "    def __init__(self, save_path):\n",
    "        self.save_path = save_path # directory to cache intermediate results\n",
    "        self.result_paths = [] # to keep track of cached files\n",
    "        \n",
    "        self.create_save_folder()\n",
    "        \n",
    "    def create_save_folder(self):\n",
    "        os.makedirs(self.save_path)\n",
    "        \n",
    "    def get_save_path(self, filename):\n",
    "        return os.path.join(self.save_path, filename)\n",
    "        \n",
    "    def cache_result(self, result, filename):\n",
    "        \"\"\"Pickles result to given filename in save_path\"\"\"\n",
    "        file_save_path = self.get_save_path(filename)\n",
    "        logger.info(f\"Saving result to {file_save_path}\")\n",
    "        with open(file_save_path, \"wb\") as f:\n",
    "            pickle.dump(result, f)\n",
    "            \n",
    "        self.result_paths.append(file_save_path)\n",
    "            \n",
    "    def load_all_results(self):\n",
    "        \"\"\"Loads all cached results and returns as a list\"\"\"\n",
    "        results = []\n",
    "        for path in self.result_paths:\n",
    "            with open(path, \"rb\") as f:\n",
    "                results.append(pickle.load(f))\n",
    "        return results\n",
    "\n",
    "\n",
    "def chunk_ts_feats(full_df, n_splits):\n",
    "    \"\"\"Run all_feats_minimal but chunk over the original df, save intermediate results to disk,\n",
    "    and concatenate all results after finished to avoid memory issues for the full data preparation.\n",
    "    \n",
    "    Note: chunks over timestamps, not array indices to reduce potential ordering bugs.\n",
    "    \n",
    "    Warning: deletes all data in full_df to help avoid memory issues\n",
    "    \"\"\"\n",
    "    include_target = True\n",
    "    all_timestamps = np.sort(full_df[\"timestamp\"].unique())\n",
    "    chunk_size = len(all_timestamps) // n_splits + 1000 # avoid rounding issue for last chunk\n",
    "    \n",
    "    result_cacher = ResultCacher(\"/tmp/feat_cache\")\n",
    "    \n",
    "    for i, time_chunk in enumerate(chunk_list(all_timestamps, chunk_size)):   \n",
    "        df_chunk = full_df.loc[full_df.timestamp.isin(time_chunk)]\n",
    "        feat_chunk = all_feats_minimal(df_chunk, include_target=True).dropna()\n",
    "        result_cacher.cache_result(feat_chunk, f\"feat_chunk_{i}.pkl\")\n",
    "        \n",
    "    full_df.drop(full_df.index, inplace=True)  # reduce memory before reloading cache\n",
    "    all_feats = result_cacher.load_all_results() # FIXME: does not release \n",
    "    return pd.concat(all_feats)\n",
    "        \n",
    "        \n",
    "\n",
    "def last_n_ts_df(df, lookback, buffer):\n",
    "    \"\"\"Returns the last rows of df where the timestamp is in the last n of all\n",
    "    timestamps. This is to concatenate with new data provided by the API so that\n",
    "    rolling calculations can be performed.\n",
    "\n",
    "    Warning: assumes df is ordered by timestamps, and could return more data than\n",
    "    requested.\n",
    "    \"\"\"\n",
    "    n_assets = 14\n",
    "    return df.iloc[-(n_assets * lookback + buffer) :]\n",
    "\n",
    "\n",
    "def drop_dup_ts_id(df):\n",
    "    \"\"\"Drops duplicated rows based on timestamp and asset_id columns, keeping the last rows found\"\"\"\n",
    "    return df.loc[~df.duplicated(subset=[\"timestamp\", \"Asset_ID\"], keep=\"first\")]\n",
    "\n",
    "\n",
    "def concat_old_new(old_data, new_data, index_check=False):\n",
    "    \"\"\"Concatenate old and new dfs for feature construction. Ensures\n",
    "    any overlapping timestamps + assetids in the old df are discarded.\n",
    "    \"\"\"\n",
    "    concat = pd.concat([old_data, new_data.drop(columns=\"row_id\")], ignore_index=True)\n",
    "    \n",
    "    if index_check:\n",
    "        return drop_dup_ts_id(concat)\n",
    "    \n",
    "    return concat\n",
    "\n",
    "\n",
    "def subset_test_index(data, orig_data):\n",
    "    \"\"\"Subset the prepred data df on the original test timestamps + assetids\"\"\"\n",
    "    orig_index = pd.MultiIndex.from_frame(orig_data[[\"timestamp\", \"Asset_ID\"]])\n",
    "    return data.loc[orig_index]\n",
    "\n",
    "\n",
    "def join_rowids(preds, orig_test):\n",
    "    \"\"\"Join our predictions df with the rowids in the supplied test data df\"\"\"\n",
    "    orig_join_on = orig_test[[\"timestamp\", \"Asset_ID\", \"row_id\"]].set_index(\n",
    "        [\"timestamp\", \"Asset_ID\"]\n",
    "    )\n",
    "    return preds.join(orig_join_on).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def _predict_loop(model, prev_data, new_data, sample_pred_df, n_to_keep, concat_check):\n",
    "    \"\"\"Function for looping over in env.iter_test():\n",
    "    - Concatenate previous + new data\n",
    "    - Cache last n rows of this df\n",
    "    - Calculate new features\n",
    "    - Drop rows to match the original training timestamps + asset ids\n",
    "    - Calculate predictions on this subset\n",
    "    - Join with the given row ids in the sample predictions df\n",
    "\n",
    "    Returns: last n rows from prev + new data, predictions df\n",
    "    \"\"\"\n",
    "    concat_data = concat_old_new(prev_data, new_data, concat_check)\n",
    "    last_n = last_n_ts_df(concat_data, n_to_keep, buffer=15)\n",
    "    feats = all_feats_minimal(concat_data, include_target=False)\n",
    "    feats = subset_test_index(feats, new_data).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    preds = model.predict(feats).rename(\"Target\").to_frame()\n",
    "    return last_n, join_rowids(preds, new_data)\n",
    "\n",
    "\n",
    "def predict_loop(model, prev_data, new_data, sample_pred_df, n_to_keep, concat_check):\n",
    "    \"\"\"Wrap around _predict_loop to catch errors. Defaults to sample predictions if we encounter\n",
    "    an error.\n",
    "    \"\"\"\n",
    "    if not in_time_limit():\n",
    "        return prev_data, sample_pred_df\n",
    "    \n",
    "    try:\n",
    "        return _predict_loop(model, prev_data, new_data, sample_pred_df, n_to_keep, concat_check)\n",
    "    except:\n",
    "        return prev_data, sample_pred_df\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "### Functions for fitting\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "def feature_names(data_cols):\n",
    "    \"\"\"Take single/multiindex columns from a pandas df of features + targets\n",
    "    and return only the feature names.\n",
    "    \"\"\"\n",
    "    if isinstance(data_cols, pd.MultiIndex):\n",
    "        data_cols = data_cols.get_level_values(0).unique()\n",
    "\n",
    "    return [k for k in data_cols if \"target\" not in k]\n",
    "\n",
    "\n",
    "def get_xy_arrays(data_df):\n",
    "    \"\"\"Returns a tuple of numpy arrays: features, scaled targets\"\"\"\n",
    "    features = feature_names(data_df.columns)\n",
    "    try:\n",
    "        target = data_df[\"scaled_target\"].values\n",
    "    except:\n",
    "        target = None\n",
    "    return data_df[features].values, target\n",
    "\n",
    "\n",
    "def scale_predictions(model, X, y_scale):\n",
    "    \"\"\"Undo the vol normalisation for the targets to get predictions\n",
    "    for actual returns.\n",
    "    \"\"\"\n",
    "    return model.predict(X) * y_scale\n",
    "\n",
    "\n",
    "def weighted_correlation(a, b, weights):\n",
    "    \"\"\"Evaluation metric copied from the discussion page\n",
    "    https://www.kaggle.com/c/g-research-crypto-forecasting/discussion/291845\n",
    "\n",
    "    Excpects columns of actual targets, predictions and asset weights\n",
    "\n",
    "    Args:\n",
    "    - a, b: the actual and predicted weights\n",
    "    - weights: the associated asset weights\n",
    "    \"\"\"\n",
    "    w = np.ravel(weights)\n",
    "    a = np.ravel(a)\n",
    "    b = np.ravel(b)\n",
    "\n",
    "    sum_w = np.sum(w)\n",
    "    mean_a = np.sum(a * w) / sum_w\n",
    "    mean_b = np.sum(b * w) / sum_w\n",
    "    var_a = np.sum(w * np.square(a - mean_a)) / sum_w\n",
    "    var_b = np.sum(w * np.square(b - mean_b)) / sum_w\n",
    "\n",
    "    cov = np.sum((a * b * w)) / np.sum(w) - mean_a * mean_b\n",
    "    corr = cov / np.sqrt(var_a * var_b)\n",
    "\n",
    "    return corr\n",
    "\n",
    "\n",
    "\n",
    "def score_from_df(pred_df, X):\n",
    "    \"\"\"Ensure indices are aligned before calculating the weighted correlation\n",
    "    on the given predictions df.\n",
    "\n",
    "    Note: the score will be nan if one df contains index values not found in the other.\n",
    "    This likely indicates a bug in generating predictions since the predictions should\n",
    "    have been calculated using X, so the indices should be the same in some order.\n",
    "    \"\"\"\n",
    "    pred_df_reindexed = pred_df.reindex(index=X.index)\n",
    "    return weighted_correlation(\n",
    "        X.target.values,\n",
    "        pred_df_reindexed.values,\n",
    "        X.target_weight.values,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def score_pool_model(model, X):\n",
    "    \"\"\"Convenience function for generating predictions and passing to score_from_df\"\"\"\n",
    "    preds = model.predict(X)\n",
    "    return score_from_df(preds, X)\n",
    "\n",
    "\n",
    "class PoolRegressor(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Helper class for fitting pool models.\n",
    "\n",
    "    Notes:\n",
    "    - This depends on the input X being a pandas df. sklearn (deliberately) tends not\n",
    "    to work well with pandas, but we don't use sklearn functionality extensively here\n",
    "    and what we do use will be okay (for indexing sklearn seems to take care of things,\n",
    "    see: https://github.com/scikit-learn/scikit-learn/blob/0d378913be6d7e485b792ea36e9268be31ed52d0/sklearn/utils/__init__.py#L307)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_model, clusters: dict):\n",
    "        self.base_model = base_model\n",
    "        self.clusters = clusters\n",
    "        self.asset_ids_ = reduce(lambda x, y: [*x, *y], clusters.values())\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, X, y=None, **fit_kwargs):\n",
    "        \"\"\"Expects a long df using the \"targets\" column as the targets, and\n",
    "        any column without \"target\" in the name is used as a feature. Fit one\n",
    "        model for each given cluster.\n",
    "\n",
    "        Note: the case where each cluster has size 1 is the single asset model.\n",
    "        \"\"\"\n",
    "        self.models_ = {}\n",
    "        for cluster, asset_ids in self.clusters.items():\n",
    "            X_subset, y_subset = get_xy_arrays(X.loc[(slice(None), list(asset_ids)), :])\n",
    "            model_clone = clone(self.base_model)\n",
    "            model_clone.fit(\n",
    "                X_subset, y_subset, **fit_kwargs\n",
    "            )  # fit separately for compatibility with Keras\n",
    "            self.models_[cluster] = model_clone\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X) -> pd.DataFrame:\n",
    "        \"\"\"Take a long df of features and return a wide df of predictions\n",
    "        with asset_ids as columns.\n",
    "        \"\"\"\n",
    "        preds = []\n",
    "        for cluster, asset_ids in self.clusters.items():\n",
    "            X_subset = X.loc[(slice(None), asset_ids), :]\n",
    "            cluster_preds = scale_predictions(\n",
    "                self.models_[cluster], get_xy_arrays(X_subset)[0], X_subset.target_scale\n",
    "            )\n",
    "            preds.append(pd.Series(cluster_preds, index=X_subset.index))\n",
    "            # asset_preds = self.models_[asset_id].predict(get_xy_arrays(X_subset)[0])\n",
    "            # asset_preds = pd.Series(asset_preds, index=X_subset.index)\n",
    "            # preds[asset_id] = asset_preds * X_subset.target_scale # scale back to returns predictions\n",
    "\n",
    "        return pd.concat(preds).reindex(index=X.index)  # same order as input df\n",
    "\n",
    "    def score(self, X, y=None):\n",
    "        \"\"\"Return the weighted correlation between all predictions\"\"\"\n",
    "        return score_pool_model(self, X)\n",
    "\n",
    "\n",
    "class PoolVotingRegressor(RegressorMixin):\n",
    "    \"\"\"Wrapper around VotingRegressor intended for use with PoolRegressors.\n",
    "    In particular:\n",
    "    - change the default scoring function to weighted regression\n",
    "    - keep the original pandas indices to predictions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, estimators, weights=None):\n",
    "        self.estimators = estimators\n",
    "        self.weights = weights\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        empty_y = np.empty_like(X.iloc[:, 0])\n",
    "        self.voting_regressor_ = VotingRegressor(\n",
    "            estimators=self.estimators, weights=self.weights\n",
    "        )\n",
    "        self.voting_regressor_.fit(X, empty_y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Adds the original pandas index of X to the output of the wrapped\n",
    "        VotingRegressor.\n",
    "        \"\"\"\n",
    "        preds = self.voting_regressor_.predict(X)\n",
    "        return pd.Series(preds, index=X.index)\n",
    "\n",
    "    def score(self, X, y=None):  # FIXME: duplication from PoolRegressor\n",
    "        \"\"\"Return the weighted correlation between all predictions\"\"\"\n",
    "        return score_pool_model(self, X)\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "### Hardcoded model config\n",
    "####################################################################################################\n",
    "\n",
    "clusters = {\n",
    "    0: (4, 8, 10, 11),\n",
    "    2: (0, 3, 12, 7), # move 7 in this cluster\n",
    "    3: (2, 5, 13),\n",
    "    4: (1, 6, 9),\n",
    "} # arbitrary cluster labels\n",
    "\n",
    "\n",
    "\n",
    "# final_allocations = {\n",
    "#     'pool_lasso': 0.13803354577335214,\n",
    "#     'pool_LGBM': 0.10673472669853526,\n",
    "#     'single_lasso': 0.16562975805884267,\n",
    "#     'single_LGBM': 0.14721038027645952,\n",
    "#     'pool_all_lasso': 0.21147431517754645,\n",
    "#     'pool_all_LGBM': 0.23091727401526385,\n",
    "# }\n",
    "\n",
    "final_allocations = {\n",
    "    'pool_LGBM': 0.20,\n",
    "    'single_LGBM': 0.30,\n",
    "    'pool_all_LGBM': 0.50,\n",
    "}\n",
    "\n",
    "\n",
    "all_assetids = list(range(14))\n",
    "\n",
    "pool_params = {\n",
    "    # \"lasso\": {\n",
    "    #     \"model\": Lasso(),\n",
    "    #     \"params\": {\"alpha\": 0.0022222223000000004, \"fit_intercept\": False},\n",
    "    # },\n",
    "    \"LGBM\": {\n",
    "        \"model\": lightgbm.LGBMRegressor(device_type=device_type),\n",
    "        \"params\": {\"learning_rate\": 0.01, \"lambda_l1\": 0.0, \"n_estimators\": 400, \"alpha\": 3}\n",
    "    },\n",
    "}\n",
    "\n",
    "single_params = {\n",
    "    # \"lasso\": {\n",
    "    #     \"model\": Lasso(),\n",
    "    #     \"params\": {\"alpha\": 0.011111111188888889, \"fit_intercept\": False},\n",
    "    # },\n",
    "    \"LGBM\": {\n",
    "        \"model\": lightgbm.LGBMRegressor(device_type=device_type),\n",
    "        \"params\": {\"learning_rate\": 0.01, \"lambda_l1\": 0.03, \"n_estimators\": 100, \"alpha\": 3}\n",
    "    },\n",
    "}\n",
    "\n",
    "all_params = {\n",
    "    # \"lasso\": {\n",
    "    #     \"model\": Lasso(),\n",
    "    #     \"params\": {\"alpha\": 0.016733333333333333, \"fit_intercept\": False},\n",
    "    # },\n",
    "    \"LGBM\": {\n",
    "        \"model\": lightgbm.LGBMRegressor(device_type=device_type),\n",
    "        \"params\": {\"learning_rate\": 0.02, \"lambda_l1\": 0.01, \"n_estimators\": 200}\n",
    "    },\n",
    "}\n",
    "\n",
    "param_dict = {\n",
    "    \"pool\": {\n",
    "        \"params\": pool_params,\n",
    "        \"clusters\": clusters,\n",
    "    },\n",
    "    \"single\": {\n",
    "        \"params\": single_params,\n",
    "        \"clusters\": {k: [k] for k in all_assetids},\n",
    "    },\n",
    "    \"pool_all\": {\n",
    "        \"params\": all_params,\n",
    "        \"clusters\": {-1: all_assetids},\n",
    "    },\n",
    "}\n",
    "\n",
    "all_models = {\n",
    "    f\"{setup}_{model_type}\": PoolRegressor(model[\"model\"].set_params(**model[\"params\"]), clusters=model_dict[\"clusters\"])\n",
    "    for setup, model_dict in param_dict.items()\n",
    "    for model_type, model in model_dict[\"params\"].items()\n",
    "}\n",
    "models_list = [(k, model) for k, model in all_models.items()]\n",
    "model_weight_list = [final_allocations[k] for k in all_models] # ensure in same order\n",
    "\n",
    "voting_model = PoolVotingRegressor(estimators=models_list, weights=model_weight_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rerun OOS predictions on test\n",
    "(commented out to avoid running during submission)\n",
    "\n",
    "Since we've changed the modelling setup we should check expectations on some OOS test set.\n",
    "\n",
    "Running the below gives roughly 2.4% correlation - this is similar to our previous score, so perhaps the extra \"market neutralisation\" step and averaging with lasso models isn't adding any value to our predictions. We didn't test this, but we ideally should."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voting_model_ = PoolVotingRegressor(estimators=models_list, weights=model_weight_list)\n",
    "\n",
    "# timestamps = np.sort(train.index.get_level_values(0).unique())\n",
    "# split_point = int(0.8 * len(timestamps))\n",
    "# train_index = timestamps[:split_point]\n",
    "# test_index = timestamps[split_point:]\n",
    "\n",
    "# voting_model_.fit(train.loc[train_index])\n",
    "\n",
    "# ad = pd.read_csv(\"asset_details.csv\").set_index(\"Asset_ID\")\n",
    "# weights = ad[\"Weight\"]\n",
    "# weights /= weights.sum()\n",
    "\n",
    "# target_weights = ad[[\"Weight\"]].rename(columns={\"Weight\": \"target_weight\"})\n",
    "# voting_model_.score(train.loc[test_index].join(target_weights, on=\"Asset_ID\"))\n",
    "# roughly 2.4%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = concat_old_new(pd.read_csv(\"train.csv\"), pd.read_csv(\"supplemental_train.csv\"), index_check=True)\n",
    "\n",
    "N_TO_KEEP = 1000\n",
    "last_n = last_n_ts_df(train_data, N_TO_KEEP, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf /tmp/feat_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "train = chunk_ts_feats(train_data, 3).replace([np.inf, -np.inf], np.nan).dropna() # FIXME: np.inf introduced, probably from std dev normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=0.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.PoolVotingRegressor at 0x7f5715b59ed0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n",
      "CPU times: user 1.84 s, sys: 12.7 ms, total: 1.85 s\n",
      "Wall time: 445 ms\n"
     ]
    }
   ],
   "source": [
    "check_index = True\n",
    "\n",
    "for (test_df, sample_prediction_df) in iter_test:\n",
    "    last_n, preds = predict_loop(voting_model, last_n, test_df, sample_prediction_df, N_TO_KEEP, check_index)\n",
    "    env.predict(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'PoolVotingRegressor' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_881/419547590.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/tmp/model.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'PoolVotingRegressor' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "with open(\"/tmp/model.pkl\", \"rb\") as f:\n",
    "    import pickle\n",
    "    a, b = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mall_feats_minimal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_target\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "\u001b[0;32mdef\u001b[0m \u001b[0mall_feats_minimal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_target\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Minimal version of all_feats\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprice_mom_windows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbar_feats_minimal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# augment in-place with bar features\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Count\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Open\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"High\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Low\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Volume\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"VWAP\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m  \u001b[0;31m# drop unused columns\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Asset_ID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'last'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mts_feats_minimal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprice_mom_windows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      /tmp/ipykernel_881/1764661174.py\n",
      "\u001b[0;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "all_feats_minimal??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_ = train_data.set_index([\"timestamp\", \"Asset_ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train_data.duplicated(subset=[\"timestamp\", \"Asset_ID\"], keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>Asset_ID</th>\n",
       "      <th>Count</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>VWAP</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1514764860</td>\n",
       "      <td>2</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2376.580000</td>\n",
       "      <td>2399.500000</td>\n",
       "      <td>2357.140000</td>\n",
       "      <td>2374.590000</td>\n",
       "      <td>1.923301e+01</td>\n",
       "      <td>2373.116392</td>\n",
       "      <td>-0.004218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1514764860</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.530000</td>\n",
       "      <td>8.530000</td>\n",
       "      <td>8.530000</td>\n",
       "      <td>8.530000</td>\n",
       "      <td>7.838000e+01</td>\n",
       "      <td>8.530000</td>\n",
       "      <td>-0.014399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1514764860</td>\n",
       "      <td>1</td>\n",
       "      <td>229.0</td>\n",
       "      <td>13835.194000</td>\n",
       "      <td>14013.800000</td>\n",
       "      <td>13666.110000</td>\n",
       "      <td>13850.176000</td>\n",
       "      <td>3.155006e+01</td>\n",
       "      <td>13827.062093</td>\n",
       "      <td>-0.014643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1514764860</td>\n",
       "      <td>5</td>\n",
       "      <td>32.0</td>\n",
       "      <td>7.659600</td>\n",
       "      <td>7.659600</td>\n",
       "      <td>7.656700</td>\n",
       "      <td>7.657600</td>\n",
       "      <td>6.626713e+03</td>\n",
       "      <td>7.657713</td>\n",
       "      <td>-0.013922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1514764860</td>\n",
       "      <td>7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25.920000</td>\n",
       "      <td>25.920000</td>\n",
       "      <td>25.874000</td>\n",
       "      <td>25.877000</td>\n",
       "      <td>1.210873e+02</td>\n",
       "      <td>25.891363</td>\n",
       "      <td>-0.008264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015107</th>\n",
       "      <td>1632182400</td>\n",
       "      <td>9</td>\n",
       "      <td>775.0</td>\n",
       "      <td>157.181571</td>\n",
       "      <td>157.250000</td>\n",
       "      <td>156.700000</td>\n",
       "      <td>156.943857</td>\n",
       "      <td>4.663725e+03</td>\n",
       "      <td>156.994319</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015108</th>\n",
       "      <td>1632182400</td>\n",
       "      <td>10</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2437.065067</td>\n",
       "      <td>2438.000000</td>\n",
       "      <td>2430.226900</td>\n",
       "      <td>2432.907467</td>\n",
       "      <td>3.975460e+00</td>\n",
       "      <td>2434.818747</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015109</th>\n",
       "      <td>1632182400</td>\n",
       "      <td>13</td>\n",
       "      <td>380.0</td>\n",
       "      <td>0.091390</td>\n",
       "      <td>0.091527</td>\n",
       "      <td>0.091260</td>\n",
       "      <td>0.091349</td>\n",
       "      <td>2.193732e+06</td>\n",
       "      <td>0.091388</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015110</th>\n",
       "      <td>1632182400</td>\n",
       "      <td>12</td>\n",
       "      <td>177.0</td>\n",
       "      <td>0.282168</td>\n",
       "      <td>0.282438</td>\n",
       "      <td>0.281842</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>1.828508e+05</td>\n",
       "      <td>0.282134</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015111</th>\n",
       "      <td>1632182400</td>\n",
       "      <td>11</td>\n",
       "      <td>48.0</td>\n",
       "      <td>232.695000</td>\n",
       "      <td>232.800000</td>\n",
       "      <td>232.240000</td>\n",
       "      <td>232.275000</td>\n",
       "      <td>1.035123e+02</td>\n",
       "      <td>232.569697</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26251918 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          timestamp  Asset_ID  Count          Open          High  \\\n",
       "0        1514764860         2   40.0   2376.580000   2399.500000   \n",
       "1        1514764860         0    5.0      8.530000      8.530000   \n",
       "2        1514764860         1  229.0  13835.194000  14013.800000   \n",
       "3        1514764860         5   32.0      7.659600      7.659600   \n",
       "4        1514764860         7    5.0     25.920000     25.920000   \n",
       "...             ...       ...    ...           ...           ...   \n",
       "2015107  1632182400         9  775.0    157.181571    157.250000   \n",
       "2015108  1632182400        10   34.0   2437.065067   2438.000000   \n",
       "2015109  1632182400        13  380.0      0.091390      0.091527   \n",
       "2015110  1632182400        12  177.0      0.282168      0.282438   \n",
       "2015111  1632182400        11   48.0    232.695000    232.800000   \n",
       "\n",
       "                  Low         Close        Volume          VWAP    Target  \n",
       "0         2357.140000   2374.590000  1.923301e+01   2373.116392 -0.004218  \n",
       "1            8.530000      8.530000  7.838000e+01      8.530000 -0.014399  \n",
       "2        13666.110000  13850.176000  3.155006e+01  13827.062093 -0.014643  \n",
       "3            7.656700      7.657600  6.626713e+03      7.657713 -0.013922  \n",
       "4           25.874000     25.877000  1.210873e+02     25.891363 -0.008264  \n",
       "...               ...           ...           ...           ...       ...  \n",
       "2015107    156.700000    156.943857  4.663725e+03    156.994319       NaN  \n",
       "2015108   2430.226900   2432.907467  3.975460e+00   2434.818747       NaN  \n",
       "2015109      0.091260      0.091349  2.193732e+06      0.091388       NaN  \n",
       "2015110      0.281842      0.282051  1.828508e+05      0.282134       NaN  \n",
       "2015111    232.240000    232.275000  1.035123e+02    232.569697       NaN  \n",
       "\n",
       "[26251918 rows x 10 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
